{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c26be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Indexed 2 chunks in 'global_rag_collection' at c:\\Users\\aniln\\Desktop\\github_celery_redis\\realtime-rag-pipeline\\chroma_db\n",
      "ðŸ“¦ Total docs in collection: 2\n",
      "ðŸ”Ž Peek at stored docs: {'ids': ['job123_0', 'job123_1'], 'embeddings': array([[ 0.0149151 , -0.01345615,  0.00756176, ..., -0.02635218,\n",
      "        -0.01427681, -0.02069877],\n",
      "       [ 0.01427765, -0.01042295, -0.015034  , ..., -0.02466744,\n",
      "        -0.01419804, -0.00069746]], shape=(2, 1536)), 'documents': ['This is the first sample chunk.', 'This is the second sample chunk.'], 'uris': None, 'included': ['metadatas', 'documents', 'embeddings'], 'data': None, 'metadatas': [{'source': 'test_source', 'chunk_id': 0, 'job_id': 'job123'}, {'source': 'test_source', 'chunk_id': 1, 'job_id': 'job123'}]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Resolve project-root-level Chroma path\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    # Works when running as a .py script\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # Fallback for notebooks / interactive mode\n",
    "    BASE_DIR = os.getcwd()\n",
    "\n",
    "PROJECT_ROOT = os.path.dirname(BASE_DIR)                     # go one level up\n",
    "CHROMA_PATH = os.path.join(PROJECT_ROOT, \"chroma_db\")         # <project_root>/chroma_db\n",
    "COLLECTION_NAME = \"global_rag_collection\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Initialize Chroma client and embeddings\n",
    "# ---------------------------------------------------------\n",
    "client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "\n",
    "openai_emb = OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=\"text-embedding-3-small\"\n",
    ")\n",
    "# ---------------------------------------------------------\n",
    "# Indexing function\n",
    "# ---------------------------------------------------------\n",
    "def index_chunks(chunks, source_name, job_id):\n",
    "    collection = client.get_or_create_collection(name=COLLECTION_NAME, embedding_function=openai_emb)\n",
    "    ids = [f\"{job_id}_{i}\" for i in range(len(chunks))]\n",
    "    metas = [\n",
    "        {\"source\": source_name, \"job_id\": job_id, \"chunk_id\": i}\n",
    "        for i in range(len(chunks))\n",
    "    ]\n",
    "    collection.add(documents=chunks, ids=ids, metadatas=metas)\n",
    "    print(f\"âœ… Indexed {len(chunks)} chunks in '{COLLECTION_NAME}' at {CHROMA_PATH}\")\n",
    "    return len(chunks)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Test block\n",
    "# ---------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    sample_chunks = [\n",
    "        \"This is the first sample chunk.\",\n",
    "        \"This is the second sample chunk.\"\n",
    "    ]\n",
    "    index_chunks(sample_chunks, source_name=\"test_source\", job_id=\"job123\")\n",
    "\n",
    "    # Verify\n",
    "    coll = client.get_collection(name=COLLECTION_NAME)\n",
    "    print(f\"ðŸ“¦ Total docs in collection: {coll.count()}\")\n",
    "    print(\"ðŸ”Ž Peek at stored docs:\", coll.peek())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ed8547f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniln\\AppData\\Local\\Temp\\ipykernel_23748\\491776981.py:32: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
      "C:\\Users\\aniln\\AppData\\Local\\Temp\\ipykernel_23748\\491776981.py:55: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vector_store.persist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Indexed 3 chunks to 'global_rag_collection' at c:\\Users\\aniln\\Desktop\\github_celery_redis\\realtime-rag-pipeline\\chroma_db\n",
      "****************************************************************************************************\n",
      "result: {'input': 'What is Graph RAG?', 'context': [Document(metadata={'chunk_id': 0, 'source': 'sample_doc', 'job_id': 'job1'}, page_content='Graph RAG combines retrieval augmented generation with knowledge graphs.'), Document(metadata={'job_id': 'job1', 'chunk_id': 1, 'source': 'sample_doc'}, page_content='LangChain simplifies building RAG pipelines using Chroma or other vector stores.'), Document(metadata={'source': 'sample_doc', 'job_id': 'job1', 'chunk_id': 2}, page_content='Chroma DB stores embeddings persistently so queries can be fast and scalable.')], 'answer': 'Graph RAG combines retrieval augmented generation with knowledge graphs to enhance the process of information retrieval and generation. It leverages knowledge graphs to improve the quality and relevance of generated content by incorporating structured data. Graph RAG aims to provide more accurate and contextually relevant information by utilizing knowledge graphs in the retrieval and generation process.'}\n",
      "Answer: Graph RAG combines retrieval augmented generation with knowledge graphs to enhance the process of information retrieval and generation. It leverages knowledge graphs to improve the quality and relevance of generated content by incorporating structured data. Graph RAG aims to provide more accurate and contextually relevant information by utilizing knowledge graphs in the retrieval and generation process.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Load environment variables\n",
    "# ---------------------------------------------------------\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Resolve project-root-level paths\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    BASE_DIR = os.getcwd()\n",
    "\n",
    "PROJECT_ROOT = os.path.dirname(BASE_DIR)\n",
    "VECTOR_STORE_PATH = os.path.join(PROJECT_ROOT, \"chroma_db\")\n",
    "COLLECTION_NAME = \"global_rag_collection\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Initialize LangChain embeddings\n",
    "# ---------------------------------------------------------\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Initialize Chroma vector store\n",
    "# ---------------------------------------------------------\n",
    "vector_store = Chroma(\n",
    "    persist_directory=VECTOR_STORE_PATH,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=COLLECTION_NAME\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Index chunks function\n",
    "# ---------------------------------------------------------\n",
    "def index_chunks(chunks, source_name, job_id):\n",
    "    ids = [f\"{job_id}_{i}\" for i in range(len(chunks))]\n",
    "    metadatas = [{\"source\": source_name, \"job_id\": job_id, \"chunk_id\": i} for i in range(len(chunks))]\n",
    "    \n",
    "    vector_store.add_texts(\n",
    "        texts=chunks,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "    vector_store.persist()\n",
    "    print(f\"âœ… Indexed {len(chunks)} chunks to '{COLLECTION_NAME}' at {VECTOR_STORE_PATH}\")\n",
    "    return len(chunks)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Retriever\n",
    "# ---------------------------------------------------------\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Prompt template\n",
    "# ---------------------------------------------------------\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LLM\n",
    "# ---------------------------------------------------------\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Chains\n",
    "# ---------------------------------------------------------\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Ask question function\n",
    "# ---------------------------------------------------------\n",
    "def ask_question(query: str) -> str:\n",
    "    result = chain.invoke({\"input\": query})\n",
    "    print(\"*\" * 100)\n",
    "    print(\"result:\", result)\n",
    "    return result.get(\"answer\", \"No answer found.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Test: index and query\n",
    "# ---------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample chunks to index\n",
    "    sample_chunks = [\n",
    "        \"Graph RAG combines retrieval augmented generation with knowledge graphs.\",\n",
    "        \"LangChain simplifies building RAG pipelines using Chroma or other vector stores.\",\n",
    "        \"Chroma DB stores embeddings persistently so queries can be fast and scalable.\"\n",
    "    ]\n",
    "    \n",
    "    index_chunks(sample_chunks, source_name=\"sample_doc\", job_id=\"job1\")\n",
    "    \n",
    "    # Ask question\n",
    "    query = \"What is Graph RAG?\"\n",
    "    answer = ask_question(query)\n",
    "    print(\"Answer:\", answer)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f36ece3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aniln\\Desktop\\github_celery_redis\\realtime-rag-pipeline\\backend\\indexing.py:19: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
      "c:\\Users\\aniln\\Desktop\\github_celery_redis\\realtime-rag-pipeline\\backend\\indexing.py:21: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Indexed 2 chunks to 'global_rag_collection' at c:\\Users\\aniln\\Desktop\\github_celery_redis\\realtime-rag-pipeline\\chroma_db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aniln\\Desktop\\github_celery_redis\\realtime-rag-pipeline\\backend\\indexing.py:36: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vector_store.persist()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from indexing import index_chunks\n",
    "chunks = [\"Graph RAG combines retrieval with knowledge graphs.\", \"LangChain simplifies RAG pipelines.\"]\n",
    "index_chunks(chunks, source_name=\"test_doc\", job_id=\"manual_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf012ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Indexed 2 chunks to 'global_rag_collection' at c:\\Users\\aniln\\Desktop\\github_celery_redis\\realtime-rag-pipeline\\chroma_db\n",
      "Total docs after indexing: 0\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "VECTOR_STORE_PATH = os.path.join(os.getcwd(), \"chroma_db\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Create / load Chroma store without specifying collection_name\n",
    "vs = Chroma(\n",
    "    persist_directory=VECTOR_STORE_PATH,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Add texts\n",
    "chunks = [\n",
    "    \"Graph RAG combines retrieval with knowledge graphs.\",\n",
    "    \"LangChain simplifies RAG pipelines.\"\n",
    "]\n",
    "vs.add_texts(texts=chunks)\n",
    "# Persist (optional, automatic in latest versions)\n",
    "vs.persist()\n",
    "\n",
    "# Reload in another instance\n",
    "vs_reload = Chroma(\n",
    "    persist_directory=VECTOR_STORE_PATH,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "print(\"Total docs after reload:\", vs_reload._collection.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4bdf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edc351b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniln\\AppData\\Local\\Temp\\ipykernel_18492\\1172387478.py:28: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 63\u001b[0m\n\u001b[0;32m     56\u001b[0m sample_chunks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph RAG combines retrieval with knowledge graphs.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLangChain simplifies RAG pipelines.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m ]\n\u001b[0;32m     60\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdfds_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sample_chunks))]\n\u001b[0;32m     61\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     62\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m123\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: i}\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mchunks\u001b[49m))\n\u001b[0;32m     64\u001b[0m ]\n\u001b[0;32m     65\u001b[0m index_chunks(sample_chunks, source_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_doc1\u001b[39m\u001b[38;5;124m\"\u001b[39m, job_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chunks' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Paths\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    BASE_DIR = os.getcwd()\n",
    "\n",
    "PROJECT_ROOT = os.path.dirname(BASE_DIR)\n",
    "VECTOR_STORE_PATH = os.path.join(PROJECT_ROOT, \"chroma_db\", \"39359794-2f90-4588-a5b4-6e7ac5f24012\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# OpenAI embeddings\n",
    "# ---------------------------------------------------------\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Initialize Chroma vector store (default collection)\n",
    "# ---------------------------------------------------------\n",
    "vector_store = Chroma(\n",
    "    persist_directory=VECTOR_STORE_PATH,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Index chunks function\n",
    "# ---------------------------------------------------------\n",
    "def index_chunks(chunks, source_name, job_id):\n",
    "    \"\"\"\n",
    "    Index a list of text chunks into Chroma.\n",
    "    Each chunk is stored with metadata: source_name, job_id, chunk_id\n",
    "    \"\"\"\n",
    "    ids = [f\"{job_id}_{i}\" for i in range(len(chunks))]\n",
    "    metadatas = [\n",
    "        {\"source\": source_name, \"job_id\": job_id, \"chunk_id\": i}\n",
    "        for i in range(len(chunks))\n",
    "    ]\n",
    "\n",
    "    vector_store.add_texts(texts=chunks, metadatas=metadatas, ids=ids)\n",
    "    print(f\"âœ… Indexed {len(chunks)} chunks at {VECTOR_STORE_PATH}\")\n",
    "    return len(chunks)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Quick test\n",
    "# ------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    sample_chunks = [\n",
    "        \"Graph RAG combines retrieval with knowledge graphs.\",\n",
    "        \"LangChain simplifies RAG pipelines.\"\n",
    "    ]\n",
    "    index_chunks(sample_chunks, source_name=\"test_doc1\", job_id=\"manual_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74adb690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs after reload: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniln\\AppData\\Local\\Temp\\ipykernel_19336\\1721986536.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vs.persist()\n"
     ]
    }
   ],
   "source": [
    "vs.persist()\n",
    "\n",
    "# Reload in another instance\n",
    "vs_reload = Chroma(\n",
    "    persist_directory=VECTOR_STORE_PATH,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "print(\"Total docs after reload:\", vs_reload._collection.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66eaa3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Grpah RAG?\n",
      "Answer: Graph RAG combines retrieval with knowledge graphs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Paths\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    BASE_DIR = os.getcwd()\n",
    "\n",
    "PROJECT_ROOT = os.path.dirname(BASE_DIR)\n",
    "VECTOR_STORE_PATH = os.path.join(PROJECT_ROOT, \"chroma_db\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Embeddings\n",
    "# ---------------------------------------------------------\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Load Chroma vector store (default collection)\n",
    "# ---------------------------------------------------------\n",
    "vector_store = Chroma(\n",
    "    persist_directory=VECTOR_STORE_PATH,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LLM and prompt\n",
    "# ---------------------------------------------------------\n",
    "system_prompt = (\n",
    "    \"You are an assistant that answers questions **only** using the provided context. \"\n",
    "    \"Do not make up answers. If the answer is not in the context, reply exactly: 'I don't know.' \"\n",
    "    \"Use a maximum of three sentences.\\n\"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system_prompt), (\"human\", \"{input}\")])\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, qa_chain)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Ask question function\n",
    "# ---------------------------------------------------------\n",
    "def ask_question(query: str) -> str:\n",
    "    result = chain.invoke({\"input\": query})\n",
    "    return result.get(\"answer\", \"No answer found.\")\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Quick test\n",
    "# ------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"What is Grpah RAG?\"\n",
    "    print(\"Question:\", query)\n",
    "    print(\"Answer:\", ask_question(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d083f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in Chroma DB: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Path to your Chroma DB\n",
    "VECTOR_STORE_PATH = \"./chroma_db\"\n",
    "\n",
    "# Embeddings\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "VECTOR_STORE_COLLECTION = \"default_collection\"\n",
    "\n",
    "vector_store = Chroma(\n",
    "    persist_directory=VECTOR_STORE_PATH,\n",
    "    collection_name=VECTOR_STORE_COLLECTION,\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "# Get total number of documents\n",
    "total_docs = vector_store._collection.count()\n",
    "print(f\"Total documents in Chroma DB: {total_docs}\")\n",
    "\n",
    "# Optional: Inspect first few documents\n",
    "results = vector_store._collection.get(include=[\"documents\", \"metadatas\"])\n",
    "for doc, meta in zip(results[\"documents\"], results[\"metadatas\"]):\n",
    "    print(meta, doc[:100])  # print first 100 chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa933ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aniln\\Desktop\\github_celery_redis\\realtime-rag-pipeline\\backend\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ed459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbef81a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a46d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "from uuid import uuid4\n",
    "\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",\n",
    ")\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./chroma_langchain_db\")\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=1,\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=2,\n",
    ")\n",
    "\\\n",
    "\n",
    "documents = [\n",
    "    document_1,\n",
    "    document_2,\n",
    "]\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d2e10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee0a57a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "channel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
